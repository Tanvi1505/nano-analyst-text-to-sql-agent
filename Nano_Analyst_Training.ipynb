{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤– Nano-Analyst: Fine-Tuning Pipeline\n",
        "\n",
        "**Portfolio Project: Private On-Device SQL Agent**\n",
        "\n",
        "This notebook fine-tunes Llama-3-8B on the Spider dataset for Text-to-SQL generation using QLoRA and Unsloth.\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ Setup Checklist\n",
        "\n",
        "Before running:\n",
        "1. âœ… **GPU Enabled**: Runtime â†’ Change runtime type â†’ T4 GPU\n",
        "2. âœ… **High RAM**: Runtime â†’ Change runtime type â†’ High-RAM\n",
        "3. âœ… **Data Ready**: Upload `train.json` and `validation.json` from Step 1\n",
        "\n",
        "---\n",
        "\n",
        "**Estimated Time:**\n",
        "- Setup: 5 minutes\n",
        "- Training: 2-3 hours (T4 GPU)\n",
        "- Total: ~3 hours"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“‹ Step 1: Verify GPU"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"GPU VERIFICATION\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
        "print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"âœ“ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âŒ NO GPU DETECTED!\")\n",
        "    print(\"   Go to: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU\")\n",
        "print(f\"{'='*70}\")"
      ],
      "metadata": {
        "id": "gpu_check"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¦ Step 2: Install Dependencies\n",
        "\n",
        "**Critical:** Install Unsloth for 2x faster training and 50% less memory usage."
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install Unsloth (optimized for Colab)\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Install training dependencies\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify installation\n",
        "import unsloth\n",
        "print(f\"âœ“ Unsloth version: {unsloth.__version__}\")\n",
        "print(\"âœ“ All dependencies installed successfully!\")"
      ],
      "metadata": {
        "id": "verify_install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ’¾ Step 3: Mount Google Drive & Upload Data\n",
        "\n",
        "**Important:** This saves your model to Google Drive so it persists after the session ends."
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directory in Drive\n",
        "import os\n",
        "project_dir = '/content/drive/MyDrive/nano-analyst'\n",
        "os.makedirs(f\"{project_dir}/data/processed\", exist_ok=True)\n",
        "os.makedirs(f\"{project_dir}/models\", exist_ok=True)\n",
        "\n",
        "print(f\"âœ“ Project directory created: {project_dir}\")\n",
        "print(\"\\nðŸ“‚ Next: Upload your data files to:\")\n",
        "print(f\"   {project_dir}/data/processed/\")\n",
        "print(\"\\n   Required files:\")\n",
        "print(\"   - train.json\")\n",
        "print(\"   - validation.json\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“¤ Upload Data Files\n",
        "\n",
        "**Option A:** Use Colab file browser (left sidebar) to upload to `/content/drive/MyDrive/nano-analyst/data/processed/`\n",
        "\n",
        "**Option B:** Run data preparation here:"
      ],
      "metadata": {
        "id": "upload_data"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option B: Prepare data directly in Colab\n",
        "!pip install datasets\n",
        "\n",
        "# Download and prepare Spider dataset\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Downloading Spider dataset from HuggingFace...\")\n",
        "spider = load_dataset(\"spider\")\n",
        "\n",
        "def format_to_alpaca(example):\n",
        "    \"\"\"Format Spider example to Alpaca instruction format.\"\"\"\n",
        "    # Build schema string\n",
        "    schema_parts = []\n",
        "    if 'db_table_names' in example and 'db_column_names' in example:\n",
        "        from collections import defaultdict\n",
        "        table_cols = defaultdict(list)\n",
        "        \n",
        "        for col_data in example['db_column_names']:\n",
        "            table_idx = col_data['table_id']\n",
        "            col_name = col_data['column_name']\n",
        "            if table_idx >= 0 and table_idx < len(example['db_table_names']):\n",
        "                table_cols[table_idx].append(col_name)\n",
        "        \n",
        "        for table_idx, table_name in enumerate(example['db_table_names']):\n",
        "            cols = table_cols.get(table_idx, [])\n",
        "            if cols:\n",
        "                cols_str = \",\\n  \".join([f\"{c} TEXT\" for c in cols])\n",
        "                schema_parts.append(f\"CREATE TABLE {table_name} (\\n  {cols_str}\\n);\")\n",
        "    \n",
        "    schema = \"\\n\\n\".join(schema_parts) if schema_parts else \"-- Schema not available --\"\n",
        "    \n",
        "    return {\n",
        "        \"instruction\": \"You are an expert SQL generator. Given a database schema and a natural language question, generate the correct SQL query to answer the question. Output only the SQL query without any explanations.\",\n",
        "        \"input\": f\"Database Schema:\\n{schema}\\n\\nQuestion: {example['question']}\",\n",
        "        \"output\": example['query']\n",
        "    }\n",
        "\n",
        "# Process train split\n",
        "print(\"\\nFormatting training data...\")\n",
        "train_formatted = [format_to_alpaca(ex) for ex in spider['train']]\n",
        "\n",
        "# Split into train/val\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(train_formatted)\n",
        "split_idx = int(len(train_formatted) * 0.9)\n",
        "train_subset = train_formatted[:split_idx]\n",
        "val_subset = train_formatted[split_idx:]\n",
        "\n",
        "# Save\n",
        "data_dir = Path(project_dir) / \"data\" / \"processed\"\n",
        "with open(data_dir / \"train.json\", 'w') as f:\n",
        "    json.dump(train_subset, f, indent=2)\n",
        "with open(data_dir / \"validation.json\", 'w') as f:\n",
        "    json.dump(val_subset, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ“ Saved {len(train_subset)} training examples\")\n",
        "print(f\"âœ“ Saved {len(val_subset)} validation examples\")\n",
        "print(f\"âœ“ Data ready at: {data_dir}\")"
      ],
      "metadata": {
        "id": "prepare_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  Step 4: Load Model & Add LoRA Adapters"
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Configuration\n",
        "max_seq_length = 1536  # Schema + Question + SQL\n",
        "dtype = None  # Auto-detect\n",
        "load_in_4bit = True  # Critical for T4 GPU\n",
        "\n",
        "# Load model (this downloads ~5GB on first run)\n",
        "print(\"Loading Llama-3-8B-Instruct (4-bit quantized)...\")\n",
        "print(\"This may take 2-3 minutes...\\n\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"âœ“ Model loaded!\\n\")\n",
        "\n",
        "# Add LoRA adapters\n",
        "print(\"Adding LoRA adapters...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,  # LoRA rank (higher = more capacity)\n",
        "    lora_alpha=64,  # Scaling factor\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # 2x memory savings\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Print parameter counts\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nâœ“ LoRA adapters configured\")\n",
        "print(f\"  Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
        "print(f\"  Total params: {total:,}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Step 5: Prepare Training Data"
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from pathlib import Path\n",
        "\n",
        "# Load data\n",
        "data_dir = Path(project_dir) / \"data\" / \"processed\"\n",
        "\n",
        "with open(data_dir / \"train.json\", 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open(data_dir / \"validation.json\", 'r') as f:\n",
        "    val_data = json.load(f)\n",
        "\n",
        "print(f\"âœ“ Loaded {len(train_data)} training examples\")\n",
        "print(f\"âœ“ Loaded {len(val_data)} validation examples\")\n",
        "\n",
        "# Format into Llama-3 chat template\n",
        "def format_prompt(example):\n",
        "    \"\"\"Format example using Llama-3-Instruct chat template.\"\"\"\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{example['instruction']}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{example['input']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{example['output']}<|eot_id|>\"\"\"\n",
        "\n",
        "print(\"\\nFormatting prompts...\")\n",
        "train_prompts = [format_prompt(ex) for ex in train_data]\n",
        "val_prompts = [format_prompt(ex) for ex in val_data]\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = Dataset.from_dict({\"text\": train_prompts})\n",
        "val_dataset = Dataset.from_dict({\"text\": val_prompts})\n",
        "\n",
        "print(\"âœ“ Datasets formatted and ready!\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLE TRAINING EXAMPLE\")\n",
        "print(\"=\"*70)\n",
        "print(train_prompts[0][:500] + \"...\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "prepare_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš€ Step 6: Configure Training & Start Fine-Tuning\n",
        "\n",
        "**This is the main training step - it will take 2-3 hours on T4 GPU.**"
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Output directory (in Google Drive for persistence)\n",
        "output_dir = f\"{project_dir}/models/nano-analyst-v1\"\n",
        "\n",
        "# Training arguments (optimized for T4)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    \n",
        "    # Batch size (T4 limit)\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,  # Effective batch = 16\n",
        "    \n",
        "    # Training schedule\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=100,\n",
        "    \n",
        "    # Logging & evaluation\n",
        "    logging_steps=10,\n",
        "    eval_steps=250,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    \n",
        "    # Optimization\n",
        "    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    \n",
        "    # Mixed precision\n",
        "    fp16=True,  # T4 doesn't support bf16\n",
        "    \n",
        "    # Checkpointing\n",
        "    save_total_limit=3,  # Keep only 3 best checkpoints\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    \n",
        "    # Reporting\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    report_to=\"none\",  # Disable WandB for simplicity\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=training_args,\n",
        "    packing=False,  # Don't pack sequences (important for SQL)\n",
        ")\n",
        "\n",
        "# Disable cache during training\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total epochs: 3\")\n",
        "print(f\"Batch size: 2\")\n",
        "print(f\"Gradient accumulation: 8\")\n",
        "print(f\"Effective batch size: 16\")\n",
        "print(f\"Learning rate: 2e-4\")\n",
        "print(f\"Max sequence length: 1536\")\n",
        "print(f\"\\nEstimated time: 2-3 hours on T4\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Start training\n",
        "print(\"\\nðŸš€ Starting training...\\n\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ’¾ Step 7: Save Fine-Tuned Model"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save LoRA adapters (lightweight - only ~200MB)\n",
        "lora_dir = f\"{output_dir}/lora_adapters\"\n",
        "\n",
        "model.save_pretrained(lora_dir)\n",
        "tokenizer.save_pretrained(lora_dir)\n",
        "\n",
        "print(f\"âœ“ LoRA adapters saved to: {lora_dir}\")\n",
        "print(f\"\\nFile size: ~200MB (much smaller than full 16GB model!)\")\n",
        "print(f\"\\nYour model is saved to Google Drive and will persist after this session ends.\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§ª Step 8: Test the Model (Quick Inference Check)"
      ],
      "metadata": {
        "id": "step8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test query\n",
        "test_schema = \"\"\"CREATE TABLE singers (\n",
        "  singer_id INTEGER,\n",
        "  name TEXT,\n",
        "  age INTEGER,\n",
        "  country TEXT\n",
        ");\"\"\"\n",
        "\n",
        "test_question = \"How many singers are from France?\"\n",
        "\n",
        "prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert SQL generator. Given a database schema and a natural language question, generate the correct SQL query to answer the question. Output only the SQL query without any explanations.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Database Schema:\n",
        "{test_schema}\n",
        "\n",
        "Question: {test_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "print(\"Testing fine-tuned model...\\n\")\n",
        "print(f\"Question: {test_question}\")\n",
        "print(f\"\\nExpected SQL: SELECT COUNT(*) FROM singers WHERE country = 'France'\\n\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.1,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "sql = generated.split(\"assistant\")[-1].strip()\n",
        "\n",
        "print(f\"Generated SQL: {sql}\")\n",
        "print(\"\\nâœ“ Model is working! Ready for Step 3 (SQL Agent with RAG)\")"
      ],
      "metadata": {
        "id": "test_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ‰ Training Complete!\n",
        "\n",
        "### What You've Accomplished:\n",
        "\n",
        "âœ… Fine-tuned Llama-3-8B on 6,000+ Text-to-SQL examples\n",
        "\n",
        "âœ… Used QLoRA for efficient 4-bit training\n",
        "\n",
        "âœ… Leveraged Unsloth for 2x faster training\n",
        "\n",
        "âœ… Saved model to Google Drive (persists after session)\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Step 3**: Build the SQLAgent class with RAG retrieval\n",
        "2. **Step 4**: Run evaluation benchmarks (Execution Accuracy)\n",
        "3. **Portfolio**: Deploy as a Streamlit app or API\n",
        "\n",
        "### Model Location:\n",
        "\n",
        "Your fine-tuned model is saved in:\n",
        "```\n",
        "/content/drive/MyDrive/nano-analyst/models/nano-analyst-v1/lora_adapters/\n",
        "```\n",
        "\n",
        "**File size:** ~200MB (LoRA adapters only)\n",
        "\n",
        "---\n",
        "\n",
        "**Questions?** Check the troubleshooting guide in `STEP2_TRAINING_GUIDE.md`"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
