{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Nano-Analyst: Evaluation on Colab\n",
    "\n",
    "Run this notebook on **Google Colab** to evaluate your fine-tuned model!\n",
    "\n",
    "**What this does:**\n",
    "1. Loads your fine-tuned model (already on Drive)\n",
    "2. Downloads Spider validation data\n",
    "3. Runs evaluation with self-correction tracking\n",
    "4. Saves results as JSON\n",
    "5. You download JSON â†’ generate dashboard on Mac\n",
    "\n",
    "**Runtime:** ~30 minutes for 100 examples on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/nano-analyst')\n",
    "\n",
    "print(\"âœ“ Mounted Drive\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Spider Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Downloading Spider validation data...\")\n",
    "\n",
    "dataset = load_dataset(\"spider\")\n",
    "validation_data = dataset['validation']\n",
    "\n",
    "print(f\"âœ“ Loaded {len(validation_data)} validation examples\")\n",
    "\n",
    "# Save to JSON\n",
    "val_json = []\n",
    "for ex in validation_data:\n",
    "    val_json.append({\n",
    "        \"db_id\": ex['db_id'],\n",
    "        \"question\": ex['question'],\n",
    "        \"query\": ex['query']\n",
    "    })\n",
    "\n",
    "Path(\"data/spider_eval\").mkdir(parents=True, exist_ok=True)\n",
    "with open(\"data/spider_eval/validation_eval.json\", 'w') as f:\n",
    "    json.dump(val_json, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved to data/spider_eval/validation_eval.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Your Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "print(\"Loading your fine-tuned model...\")\n",
    "\n",
    "max_seq_length = 1536\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"Loading LoRA adapters...\")\n",
    "\n",
    "# Load your trained LoRA adapters\n",
    "lora_path = \"/content/drive/MyDrive/nano-analyst/models/nano-analyst-v1/lora_adapters\"\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Load trained weights\n",
    "model.load_adapter(lora_path)\n",
    "\n",
    "# Set to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"âœ“ Model loaded successfully!\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Simple Evaluation (No Database Execution)\n",
    "\n",
    "Since we don't have the actual databases on Colab, we'll do **SQL generation evaluation**:\n",
    "- Test if model generates valid SQL\n",
    "- Track self-correction attempts\n",
    "- Compare generated SQL to gold SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def clean_sql(sql):\n",
    "    \"\"\"Clean generated SQL.\"\"\"\n",
    "    sql = re.sub(r'```sql\\s*', '', sql)\n",
    "    sql = re.sub(r'```\\s*', '', sql)\n",
    "    sql = ' '.join(sql.split())\n",
    "    sql = sql.rstrip()\n",
    "    if not sql.endswith(';'):\n",
    "        sql += ';'\n",
    "    return sql\n",
    "\n",
    "def generate_sql(question, schema=\"\"):\n",
    "    \"\"\"Generate SQL using fine-tuned model.\"\"\"\n",
    "    \n",
    "    instruction = (\n",
    "        \"You are an expert SQL generator. Given a database schema and a natural language question, \"\n",
    "        \"generate the correct SQL query to answer the question. \"\n",
    "        \"Output only the SQL query without any explanations.\"\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Database Schema:\n",
    "{schema if schema else \"-- Schema not available --\"}\n",
    "\n",
    "Question: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    sql = generated.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    return clean_sql(sql)\n",
    "\n",
    "print(\"âœ“ Evaluation functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Evaluation (100 Examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "with open(\"data/spider_eval/validation_eval.json\", 'r') as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "# Limit to first 100 examples for speed\n",
    "MAX_EXAMPLES = 100\n",
    "test_data = validation_data[:MAX_EXAMPLES]\n",
    "\n",
    "print(f\"Evaluating on {len(test_data)} examples...\")\n",
    "print(\"This will take ~15-20 minutes on T4 GPU\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, example in enumerate(tqdm(test_data), 1):\n",
    "    question = example['question']\n",
    "    gold_sql = example['query']\n",
    "    db_id = example['db_id']\n",
    "    \n",
    "    # Generate SQL\n",
    "    try:\n",
    "        predicted_sql = generate_sql(question)\n",
    "        \n",
    "        # Simple string match (not execution match)\n",
    "        # Normalize both for comparison\n",
    "        gold_normalized = ' '.join(gold_sql.lower().split())\n",
    "        pred_normalized = ' '.join(predicted_sql.lower().split())\n",
    "        \n",
    "        exact_match = gold_normalized == pred_normalized\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"gold_sql\": gold_sql,\n",
    "            \"predicted_sql\": predicted_sql,\n",
    "            \"database\": db_id,\n",
    "            \"exact_match\": exact_match,\n",
    "            \"success\": True\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"gold_sql\": gold_sql,\n",
    "            \"predicted_sql\": None,\n",
    "            \"database\": db_id,\n",
    "            \"exact_match\": False,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "print(f\"\\nâœ“ Evaluation complete!\")\n",
    "\n",
    "# Calculate metrics\n",
    "total = len(results)\n",
    "successful = sum(1 for r in results if r['success'])\n",
    "exact_matches = sum(1 for r in results if r.get('exact_match', False))\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total examples: {total}\")\n",
    "print(f\"  Successful generations: {successful} ({successful/total:.1%})\")\n",
    "print(f\"  Exact SQL matches: {exact_matches} ({exact_matches/total:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"evaluation_results/colab\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save detailed results\n",
    "results_path = output_dir / \"detailed_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved detailed results: {results_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    \"total_examples\": total,\n",
    "    \"successful_generations\": successful,\n",
    "    \"success_rate\": successful / total,\n",
    "    \"exact_match_accuracy\": exact_matches / total,\n",
    "    \"exact_matches\": exact_matches\n",
    "}\n",
    "\n",
    "metrics_path = output_dir / \"metrics.json\"\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Saved metrics: {metrics_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOAD THESE FILES TO YOUR MAC:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"1. {results_path}\")\n",
    "print(f\"2. {metrics_path}\")\n",
    "print(f\"\\nThen run on Mac:\")\n",
    "print(f\"  python3 generate_dashboard.py\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Sample Results Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample Results (First 5):\\n\")\n",
    "\n",
    "for i, result in enumerate(results[:5], 1):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"  Question: {result['question']}\")\n",
    "    print(f\"  Gold SQL: {result['gold_sql']}\")\n",
    "    print(f\"  Predicted: {result['predicted_sql']}\")\n",
    "    print(f\"  Match: {'âœ“' if result.get('exact_match') else 'âœ—'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Done!\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "1. **Download the JSON files** from Drive:\n",
    "   - `evaluation_results/colab/detailed_results.json`\n",
    "   - `evaluation_results/colab/metrics.json`\n",
    "\n",
    "2. **Copy them to your Mac:**\n",
    "   ```bash\n",
    "   # Put them in:\n",
    "   ~/nano-analyst/evaluation_results/\n",
    "   ```\n",
    "\n",
    "3. **Generate dashboard on Mac:**\n",
    "   ```bash\n",
    "   cd ~/nano-analyst\n",
    "   python3 generate_dashboard.py\n",
    "   open evaluation_results/dashboard.html\n",
    "   ```\n",
    "\n",
    "**Your evaluation is complete!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
